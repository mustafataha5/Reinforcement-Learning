{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNYkOT28orHkLFp9WuybW35"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"GbE0DDeh9eL-","executionInfo":{"status":"ok","timestamp":1753878126243,"user_tz":-180,"elapsed":409,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"outputs":[],"source":["import gymnasium as gym\n","import nbformat\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2DUt9sgR9wD_","executionInfo":{"status":"ok","timestamp":1753878141543,"user_tz":-180,"elapsed":15304,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}},"outputId":"617b6aa1-9742-438d-e014-b57969ba2c6d"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["class_path= \"/content/drive/MyDrive/husob/06 ReinForcement Learning/004/Drone_env.ipynb\""],"metadata":{"id":"HVf0f9f390xd","executionInfo":{"status":"ok","timestamp":1753878141565,"user_tz":-180,"elapsed":19,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","source":["with open(class_path, 'r') as f:\n","    nb = nbformat.read(f, as_version=4)"],"metadata":{"id":"5cwHia8I-Aov","executionInfo":{"status":"ok","timestamp":1753878143366,"user_tz":-180,"elapsed":1818,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["for cell in nb.cells:\n","    if cell.cell_type == 'code':\n","       try:\n","        exec(cell.source, globals())\n","       except Exception as e:\n","        print(f\"Error executing cell: {e}\")\n",""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IOHpEX9a-EjV","executionInfo":{"status":"ok","timestamp":1753878144253,"user_tz":-180,"elapsed":886,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}},"outputId":"bab82c86-358a-43ec-e423-7cd98caa8073"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Error executing cell: not enough values to unpack (expected 5, got 4)\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gymnasium/utils/passive_env_checker.py:212: DeprecationWarning:\n","\n","\u001b[33mWARN: Core environment is written in old step API which returns one bool instead of two. It is recommended to rewrite the environment with new step API. \u001b[0m\n","\n"]}]},{"cell_type":"code","source":[" # الأبنية\n","buildings = [\n","      {'pos': [3, 3, 0], 'size': [5, 5, 5]},\n","      {'pos': [8, 8, 0], 'size': [1, 1, 1]}\n","  ]\n","  # المناطق الممنوعة\n","prohibited_zones = [\n","      {'pos': [0, 2, 0], 'size': [2, 4, 4]},\n","      {'pos': [8, 1, 0], 'size': [2, 2, 5]}\n","  ]\n","  # البداية\n","start_position = [1, 9, 0]\n","  # الهدف\n","goal_position = [9, 0, 8]\n","\n","  # إنشاء البيئة\n","env = gym.make(\"Drone-v0\",\n","               grid_size=10,\n","               start_position=start_position,buildings=buildings,\n","               prohibited_zones=prohibited_zones ,\n","               max_episode_steps=400)\n","\n","env.reset()\n","raw_env = env.unwrapped\n","raw_env.set_goal(goal_position )\n","\n","\n","\n","# # الإظهار\n","# env.render()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0qn9dPFB-mOv","executionInfo":{"status":"ok","timestamp":1753878144260,"user_tz":-180,"elapsed":5,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}},"outputId":"06689713-ff09-4041-cb51-7b8af3545059"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gymnasium/envs/registration.py:512: DeprecationWarning:\n","\n","\u001b[33mWARN: The environment Drone-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n","\n"]}]},{"cell_type":"code","source":["import numpy as np\n","import time\n","from tqdm import trange\n","from IPython.display import clear_output\n","\n","# --- Q-Learning Agent ---\n","class QLearningAgent:\n","    \"\"\"\n","    A Q-Learning agent for discrete observation and action spaces.\n","    \"\"\"\n","    def __init__(self, observation_space_size, action_space_size, learning_rate=0.1, discount_factor=0.99, epsilon=1.0):\n","        self.q_table = np.zeros((observation_space_size, action_space_size))\n","        self.learning_rate = learning_rate  # Alpha\n","        self.discount_factor = discount_factor  # Gamma\n","        self.epsilon = epsilon  # Exploration factor\n","\n","    def choose_action(self, state):\n","        \"\"\"\n","        Chooses an action using an epsilon-greedy policy.\n","        \"\"\"\n","        if np.random.rand() < self.epsilon:\n","            return np.random.randint(self.q_table.shape[1])  # Explore\n","        else:\n","            return np.argmax(self.q_table[state])  # Exploit\n","\n","    def learn(self, state, action, reward, next_state):\n","        \"\"\"\n","        Q(s,a) = Q(s,a) + alpha * (reward + gamma * max(Q(s',a')) - Q(s,a))\n","        \"\"\"\n","        old_value = self.q_table[state, action]\n","        next_max = np.max(self.q_table[next_state])\n","        new_value = old_value + self.learning_rate * (reward + self.discount_factor * next_max - old_value)\n","        self.q_table[state, action] = new_value\n","\n","\n","# --- Training Function ---\n","def train_q_learning(env, agent, num_episodes=10000, epsilon_decay_rate=0.999, min_epsilon=0.01,\n","                     render_each_n_episodes=1000, seed=None):\n","    \"\"\"\n","    Trains a Q-Learning agent in the given environment.\n","    \"\"\"\n","    if seed is not None:\n","        np.random.seed(seed)\n","        env.reset(seed=seed)\n","\n","    rewards_per_episode = []\n","    best_q_table = None\n","    best_avg_reward = float('-inf')\n","\n","    print(f\"Starting Q-Learning training for {num_episodes} episodes...\")\n","\n","    for episode in trange(num_episodes):\n","        state, _ = env.reset()\n","        done = False\n","        total_episode_reward = 0\n","\n","        while not done:\n","            action = agent.choose_action(state)\n","            next_state, reward, terminated, truncated, _ = env.step(action)\n","\n","            agent.learn(state, action, reward, next_state)\n","            state = next_state\n","            total_episode_reward += reward\n","            done = terminated or truncated\n","\n","        # Update epsilon\n","        agent.epsilon = max(min_epsilon, agent.epsilon * epsilon_decay_rate)\n","        rewards_per_episode.append(total_episode_reward)\n","\n","        # Track best Q-table\n","        if episode >= 100:\n","            avg_reward = np.mean(rewards_per_episode[-100:])\n","            if avg_reward > best_avg_reward:\n","                best_avg_reward = avg_reward\n","                best_q_table = agent.q_table.copy()\n","        # clear screen very 100000\n","        if (episode + 1) % 20000 == 0:\n","          clear_output(wait=True)\n","        # Logging\n","        if (episode + 1) % 1000 == 0:\n","\n","            avg_reward = np.mean(rewards_per_episode[-100:])\n","            print(f\"Episode {episode + 1}: Avg Reward (last 100) = {avg_reward:.2f}, Epsilon = {agent.epsilon:.4f}\")\n","\n","        # # Rendering\n","        # if (episode + 1) % render_each_n_episodes == 0:\n","        #     print(f\"\\n--- Rendering Episode {episode + 1} ---\")\n","        #     clear_output(wait=True)\n","        #     env.render()\n","        #     time.sleep(2)\n","\n","    print(\"\\nTraining complete!\")\n","    return {\n","        \"rewards\": rewards_per_episode,\n","        \"q_table\": agent.q_table,\n","        \"best_q_table\": best_q_table\n","    }\n","\n","\n","\n"],"metadata":{"id":"x_EchItF-sHU","executionInfo":{"status":"ok","timestamp":1753878144266,"user_tz":-180,"elapsed":5,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":7,"outputs":[]},{"cell_type":"code","source":["\n","agent = QLearningAgent(observation_space_size=env.observation_space.n, action_space_size=env.action_space.n)\n","ans = train_q_learning(env,\n","                agent,\n","                 num_episodes=2_000_000,\n","                 epsilon_decay_rate=0.999995,\n","                 min_epsilon=0.01,\n","                 render_each_n_episodes=50000\n","                 )\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"collapsed":true,"id":"YUK6hVkiALz-","executionInfo":{"status":"ok","timestamp":1753880145166,"user_tz":-180,"elapsed":1215868,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}},"outputId":"9353468d-6eda-40ad-a747-15842b30ed1c"},"execution_count":14,"outputs":[{"output_type":"stream","name":"stderr","text":["\r100%|██████████| 2000000/2000000 [20:15<00:00, 1645.12it/s]"]},{"output_type":"stream","name":"stdout","text":["Episode 2000000: Avg Reward (last 100) = 975.20, Epsilon = 0.0100\n","\n","Training complete!\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import pickle\n","def save_q_table_and_env(q_table,best_q_table,env , q_filename,best_q_filename,enV_filename):\n","    with open(q_filename, 'wb') as f:\n","        pickle.dump(q_table, f)\n","    with open(best_q_filename, 'wb') as f:\n","        pickle.dump(best_q_table, f)\n","    with open(enV_filename, 'wb') as f:\n","        pickle.dump(env, f)\n"],"metadata":{"id":"aBvTqCOIDCSV","executionInfo":{"status":"ok","timestamp":1753880148580,"user_tz":-180,"elapsed":9,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["working_dir = \"/content/drive/MyDrive/husob/06 ReinForcement Learning/004/\"\n","q_table_name = working_dir + \"q_table1.pkl\"\n","best_q_table_name = working_dir + \"best_q_table1.pkl\"\n","env_name = working_dir + \"env1.pkl\"\n","save_q_table_and_env(ans[\"q_table\"],ans[\"best_q_table\"],env,q_table_name,best_q_table_name,env_name)\n","# save_q_table_and_env(,env,best_q_table_name,env_name)"],"metadata":{"id":"PhAJSv7vDCHg","executionInfo":{"status":"ok","timestamp":1753880150109,"user_tz":-180,"elapsed":334,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["#  # الأبنية\n","# buildings = [\n","#       {'pos': [3, 3, 0], 'size': [5, 5, 5]},\n","#       {'pos': [8, 8, 0], 'size': [1, 1, 1]}\n","#   ]\n","#   # المناطق الممنوعة\n","# prohibited_zones = [\n","#       {'pos': [0, 2, 0], 'size': [2, 4, 4]},\n","#       {'pos': [8, 1, 0], 'size': [2, 2, 5]}\n","#   ]\n","#   # البداية\n","# start_position = [1, 9, 0]\n","#   # الهدف\n","# goal_position = [9, 0, 8]\n","\n","#   # إنشاء البيئة\n","# env2 = gym.make(\"Drone-v1\",\n","#                grid_size=10,\n","#                start_position=start_position,buildings=buildings,\n","#                prohibited_zones=prohibited_zones ,\n","#                max_episode_steps=200)\n","\n","# env2.reset()\n","# raw_env2 = env.unwrapped\n","# raw_env2.set_goal(goal_position )\n"],"metadata":{"id":"bPhQ5YzyA8cv","executionInfo":{"status":"aborted","timestamp":1753878230291,"user_tz":-180,"elapsed":104563,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# agent2 = QLearningAgent(observation_space_size=env2.observation_space.n, action_space_size=env2.action_space.n)\n","# ans2 = train_q_learning(\n","#                 env2,\n","#                 agent2,\n","#                  num_episodes=2_000_000,\n","#                  epsilon_decay_rate=0.999992,\n","#                  min_epsilon=0.01,\n","#                 #  render_each_n_episodes=50000\n","#                  )"],"metadata":{"id":"Z5EbyaXGLkQf","executionInfo":{"status":"aborted","timestamp":1753878230292,"user_tz":-180,"elapsed":104561,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# working_dir = \"/content/drive/MyDrive/husob/06 ReinForcement Learning/004/\"\n","# q_table_name = working_dir + \"q_table2.pkl\"\n","# env_name = working_dir + \"env2.pkl\"\n","# save_q_table_and_env(ans2[\"q_table\"],env2,q_table_name,env_name)"],"metadata":{"id":"Rc6AUKbYEZg2","executionInfo":{"status":"aborted","timestamp":1753878230294,"user_tz":-180,"elapsed":104561,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Q2KxqKBSL-wT","executionInfo":{"status":"aborted","timestamp":1753878230295,"user_tz":-180,"elapsed":104560,"user":{"displayName":"mustafa taha","userId":"16679109722330067367"}}},"execution_count":null,"outputs":[]}]}