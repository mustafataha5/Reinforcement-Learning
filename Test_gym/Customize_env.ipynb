{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9a1997e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "class GridWorldEnv(gym.Env):\n",
    "\n",
    "    def __init__(self, size: int = 5, reward_scale: float = 1.0, step_penalty: float = 0.0):\n",
    " \n",
    "        self.reward_scale = reward_scale\n",
    "        self.step_penalty = step_penalty\n",
    "        # The size of the square grid (5x5 by default)\n",
    "        self.size = size\n",
    "\n",
    "        # Initialize positions - will be set randomly in reset()\n",
    "        # Using -1,-1 as \"uninitialized\" state\n",
    "        self._agent_location = np.array([-1, -1], dtype=np.int32)\n",
    "        self._target_location = np.array([-1, -1], dtype=np.int32)\n",
    "\n",
    "        # Define what the agent can observe\n",
    "        # Dict space gives us structured, human-readable observations\n",
    "        self.observation_space = gym.spaces.Dict(\n",
    "            {\n",
    "                \"agent\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),   # [x, y] coordinates\n",
    "                \"target\": gym.spaces.Box(0, size - 1, shape=(2,), dtype=int),  # [x, y] coordinates\n",
    "            }\n",
    "        )\n",
    "\n",
    "        # Define what actions are available (4 directions)\n",
    "        self.action_space = gym.spaces.Discrete(4)\n",
    "\n",
    "        # Map action numbers to actual movements on the grid\n",
    "        # This makes the code more readable than using raw numbers\n",
    "        self._action_to_direction = {\n",
    "            0: np.array([1, 0]),   # Move right (positive x)\n",
    "            1: np.array([0, 1]),   # Move up (positive y)\n",
    "            2: np.array([-1, 0]),  # Move left (negative x)\n",
    "            3: np.array([0, -1]),  # Move down (negative y)\n",
    "        }\n",
    "    \n",
    "    def _get_obs(self):\n",
    "        \"\"\"Convert internal state to observation format.\n",
    "\n",
    "        Returns:\n",
    "            dict: Observation with agent and target positions\n",
    "        \"\"\"\n",
    "        return {\"agent\": self._agent_location, \"target\": self._target_location}    \n",
    "    \n",
    "    def _get_info(self):\n",
    "        \"\"\"Compute auxiliary information for debugging.\n",
    "\n",
    "        Returns:\n",
    "            dict: Info with distance between agent and target\n",
    "        \"\"\"\n",
    "        return {\n",
    "            \"distance\": np.linalg.norm(\n",
    "                self._agent_location - self._target_location, ord=1\n",
    "            )\n",
    "        }\n",
    "    \n",
    "    def reset(self, seed: Optional[int] = None, options: Optional[dict] = None):\n",
    "        \"\"\"Start a new episode.\n",
    "\n",
    "        Args:\n",
    "            seed: Random seed for reproducible episodes\n",
    "            options: Additional configuration (unused in this example)\n",
    "\n",
    "        Returns:\n",
    "            tuple: (observation, info) for the initial state\n",
    "        \"\"\"\n",
    "        # IMPORTANT: Must call this first to seed the random number generator\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        # Randomly place the agent anywhere on the grid\n",
    "        self._agent_location = self.np_random.integers(0, self.size, size=2, dtype=int)\n",
    "\n",
    "        # Randomly place target, ensuring it's different from agent position\n",
    "        self._target_location = self._agent_location\n",
    "        while np.array_equal(self._target_location, self._agent_location):\n",
    "            self._target_location = self.np_random.integers(\n",
    "                0, self.size, size=2, dtype=int\n",
    "            )\n",
    "\n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, info\n",
    "    \n",
    "    def step(self, action):\n",
    "        \"\"\"Execute one timestep within the environment.\n",
    "\n",
    "        Args:\n",
    "            action: The action to take (0-3 for directions)\n",
    "\n",
    "        Returns:\n",
    "            tuple: (observation, reward, terminated, truncated, info)\n",
    "        \"\"\"\n",
    "        # Map the discrete action (0-3) to a movement direction\n",
    "        direction = self._action_to_direction[action]\n",
    "\n",
    "        # Update agent position, ensuring it stays within grid bounds\n",
    "        # np.clip prevents the agent from walking off the edge\n",
    "        self._agent_location = np.clip(\n",
    "            self._agent_location + direction, 0, self.size - 1\n",
    "        )\n",
    "\n",
    "        # Check if agent reached the target\n",
    "        terminated = np.array_equal(self._agent_location, self._target_location)\n",
    "\n",
    "        # We don't use truncation in this simple environment\n",
    "        # (could add a step limit here if desired)\n",
    "        truncated = False\n",
    "\n",
    "        # Simple reward structure: +1 for reaching target, 0 otherwise\n",
    "        # Alternative: could give small negative rewards for each step to encourage efficiency\n",
    "        #reward = 1 if terminated else 0\n",
    "        # Option 1: Small step penalty to encourage efficiency\n",
    "        # reward = 1 if terminated else -0.01\n",
    "\n",
    "        # Option 2: Distance-based reward shaping\n",
    "        # distance = np.linalg.norm(self._agent_location - self._target_location)\n",
    "        # reward = 1 if terminated else -0.1 * distance\n",
    "        # Flexible reward calculation\n",
    "        if terminated:\n",
    "            reward = self.reward_scale  # Success reward\n",
    "        else:\n",
    "            reward = -self.step_penalty  # Step penalty (0 by default)\n",
    "        \n",
    "        observation = self._get_obs()\n",
    "        info = self._get_info()\n",
    "\n",
    "        return observation, reward, terminated, truncated, info\n",
    "    \n",
    "    def render(self):\n",
    "        \"\"\"Render the environment for human viewing.\"\"\"\n",
    "        if self.render_mode == \"human\":\n",
    "            # Print a simple ASCII representation\n",
    "            for y in range(self.size - 1, -1, -1):  # Top to bottom\n",
    "                row = \"\"\n",
    "                for x in range(self.size):\n",
    "                    if np.array_equal([x, y], self._agent_location):\n",
    "                        row += \"A \"  # Agent\n",
    "                    elif np.array_equal([x, y], self._target_location):\n",
    "                        row += \"T \"  # Target\n",
    "                    else:\n",
    "                        row += \". \"  # Empty\n",
    "                print(row)\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e82d9747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the environment so we can create it with gym.make()\n",
    "gym.register(\n",
    "    id=\"gymnasium_env/GridWorld-v0\",\n",
    "    entry_point=GridWorldEnv,\n",
    "    max_episode_steps=300,  # Prevent infinite episodes\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24eba851",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "env = gym.make(\"gymnasium_env/GridWorld-v0\")\n",
    "env1 = gym.make(\"gymnasium_env/GridWorld-v0\", size=10)\n",
    "env.unwrapped.size,env1.unwrapped.size\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0fafc8af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment passes all checks!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\ai_env\\lib\\site-packages\\gymnasium\\utils\\env_checker.py:384: UserWarning: \u001b[33mWARN: The environment (<TimeLimit<OrderEnforcing<PassiveEnvChecker<GridWorldEnv<gymnasium_env/GridWorld-v0>>>>>) is different from the unwrapped version (<GridWorldEnv<gymnasium_env/GridWorld-v0>>). This could effect the environment checker as the environment most likely has a wrapper applied to it. We recommend using the raw environment for `check_env` using `env.unwrapped`.\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "from gymnasium.utils.env_checker import check_env\n",
    "\n",
    "# This will catch many common issues\n",
    "try:\n",
    "    check_env(env)\n",
    "    print(\"Environment passes all checks!\")\n",
    "except Exception as e:\n",
    "    print(f\"Environment has issues: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95ce098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting position - Agent: [0 3], Target: [3 2]\n",
      "Action 0: [0 3] -> [1 3], reward=-0.223606797749979\n",
      "Action 1: [1 3] -> [1 4], reward=-0.28284271247461906\n",
      "Action 2: [1 4] -> [0 4], reward=-0.36055512754639896\n",
      "Action 3: [0 4] -> [0 3], reward=-0.316227766016838\n"
     ]
    }
   ],
   "source": [
    "# Test specific action sequences to verify behavior\n",
    "env = gym.make(\"gymnasium_env/GridWorld-v0\")\n",
    "obs, info = env.reset(seed=42)  # Use seed for reproducible testing\n",
    "\n",
    "print(f\"Starting position - Agent: {obs['agent']}, Target: {obs['target']}\")\n",
    "\n",
    "# Test each action type\n",
    "actions = [0, 1, 2, 3]  # right, up, left, down\n",
    "for action in actions:\n",
    "    old_pos = obs['agent'].copy()\n",
    "    obs, reward, terminated, truncated, info = env.step(action)\n",
    "    new_pos = obs['agent']\n",
    "    print(f\"Action {action}: {old_pos} -> {new_pos}, reward={reward}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71b4133",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
