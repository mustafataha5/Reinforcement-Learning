{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5e216a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\user\\anaconda3\\envs\\ai_env\\lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import gymnasium\n",
    "import flappy_bird_gymnasium\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize\n",
    "from stable_baselines3.common.monitor import Monitor\n",
    "\n",
    "# Recreate environment\n",
    "env = gymnasium.make(\"FlappyBird-v0\", render_mode=\"rgb_array\", use_lidar=True)\n",
    "env = Monitor(env)\n",
    "env = DummyVecEnv([lambda: env])\n",
    "env = VecNormalize.load(\"ppo_flappy_2000000_vecnormalize.pkl\", env)\n",
    "env.training = False\n",
    "env.norm_reward = False\n",
    "\n",
    "# Load model\n",
    "total_timesteps = 2_000_000\n",
    "model_name = f\"ppo_flappy_{total_timesteps}\"\n",
    "model = PPO.load(model_name, env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8236a18b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé• Recording 5 episodes of the trained agent...\n",
      "MoviePy - Building video videos/ppo_flappy_2000000_episode_1.mp4.\n",
      "MoviePy - Writing video videos/ppo_flappy_2000000_episode_1.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready videos/ppo_flappy_2000000_episode_1.mp4\n",
      "‚úÖ Saved video for Episode 1 - Reward: -1.5000016689300537, Steps: 50\n",
      "MoviePy - Building video videos/ppo_flappy_2000000_episode_2.mp4.\n",
      "MoviePy - Writing video videos/ppo_flappy_2000000_episode_2.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready videos/ppo_flappy_2000000_episode_2.mp4\n",
      "‚úÖ Saved video for Episode 2 - Reward: -1.5000016689300537, Steps: 50\n",
      "MoviePy - Building video videos/ppo_flappy_2000000_episode_3.mp4.\n",
      "MoviePy - Writing video videos/ppo_flappy_2000000_episode_3.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready videos/ppo_flappy_2000000_episode_3.mp4\n",
      "‚úÖ Saved video for Episode 3 - Reward: -1.5000016689300537, Steps: 50\n",
      "MoviePy - Building video videos/ppo_flappy_2000000_episode_4.mp4.\n",
      "MoviePy - Writing video videos/ppo_flappy_2000000_episode_4.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready videos/ppo_flappy_2000000_episode_4.mp4\n",
      "‚úÖ Saved video for Episode 4 - Reward: 4.299995422363281, Steps: 102\n",
      "MoviePy - Building video videos/ppo_flappy_2000000_episode_5.mp4.\n",
      "MoviePy - Writing video videos/ppo_flappy_2000000_episode_5.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done !\n",
      "MoviePy - video ready videos/ppo_flappy_2000000_episode_5.mp4\n",
      "‚úÖ Saved video for Episode 5 - Reward: 170.90122985839844, Steps: 1990\n",
      "\n",
      "üìä Summary of Evaluation Episodes:\n",
      "Episode 1: Reward = -1.50, Steps = 50\n",
      "Episode 2: Reward = -1.50, Steps = 50\n",
      "Episode 3: Reward = -1.50, Steps = 50\n",
      "Episode 4: Reward = 4.30, Steps = 102\n",
      "Episode 5: Reward = 170.90, Steps = 1990\n",
      "\n",
      "‚úÖ Average Reward: 34.14 ¬± 68.42\n",
      "‚úÖ Average Steps: 448.40 ¬± 771.06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from moviepy import ImageSequenceClip\n",
    "import os\n",
    "\n",
    "print(\"üé• Recording 5 episodes of the trained agent...\")\n",
    "\n",
    "reward_queue = []\n",
    "time_queue = []\n",
    "video_output_dir = \"videos\"\n",
    "os.makedirs(video_output_dir, exist_ok=True)\n",
    "\n",
    "n_episodes = 5\n",
    "\n",
    "for episode in range(1, n_episodes + 1):\n",
    "    obs = env.reset()\n",
    "    done = [False]\n",
    "    frames = []\n",
    "    total_reward = 0\n",
    "    steps = 0\n",
    "\n",
    "    while not done[0]:\n",
    "        action, _states = model.predict(obs, deterministic=True)\n",
    "        obs, reward, done, info = env.step(action)\n",
    "        total_reward += reward[0]\n",
    "        steps += 1\n",
    "\n",
    "        frame = env.envs[0].render()\n",
    "        if isinstance(frame, np.ndarray) and frame.shape[-1] == 3:\n",
    "            frames.append(frame)\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Episode {episode}: Skipping a malformed frame at step {steps}.\")\n",
    "\n",
    "    # Track stats\n",
    "    reward_queue.append(total_reward)\n",
    "    time_queue.append(steps)\n",
    "\n",
    "    # Save video\n",
    "    if frames:\n",
    "        clip = ImageSequenceClip(frames, fps=30)\n",
    "        video_path = f\"{video_output_dir}/{model_name}_episode_{episode}.mp4\"\n",
    "        clip.write_videofile(video_path, fps=30)\n",
    "        print(f\"‚úÖ Saved video for Episode {episode} - Reward: {total_reward}, Steps: {steps}\")\n",
    "    else:\n",
    "        print(f\"‚ö†Ô∏è Episode {episode}: No valid frames captured.\")\n",
    "\n",
    "# After all episodes\n",
    "print(\"\\nüìä Summary of Evaluation Episodes:\")\n",
    "for i, (r, t) in enumerate(zip(reward_queue, time_queue), start=1):\n",
    "    print(f\"Episode {i}: Reward = {r:.2f}, Steps = {t}\")\n",
    "\n",
    "print(f\"\\n‚úÖ Average Reward: {np.mean(reward_queue):.2f} ¬± {np.std(reward_queue):.2f}\")\n",
    "print(f\"‚úÖ Average Steps: {np.mean(time_queue):.2f} ¬± {np.std(time_queue):.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bb8d4ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
